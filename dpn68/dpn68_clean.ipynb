{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaSBI1FrBPbO"
   },
   "source": [
    "# Drive and env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4-rDTVFBRs-",
    "outputId": "e32f6896-60ab-431b-fa33-0238107e78b4"
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Mo5mami/wtfml.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0vy-CXECUEx",
    "outputId": "1496a83e-7f3d-422c-cfb0-09aeab5b7ce5"
   },
   "outputs": [],
   "source": [
    "%pip install torchaudio librosa pretrainedmodels albumentations==0.4.6 imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAavaqBYGuXU",
    "outputId": "5727894b-b21c-46cc-eaa6-22f566da19fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.7.0\r\n",
      "torchaudio==0.7.0a0+ac17b64\r\n",
      "torchtext==0.8.0a0+cd6902d\r\n",
      "torchvision==0.8.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldjVWLvw-to9",
    "outputId": "bf9c9603-9c22-423a-a81f-78f00f597677"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import librosa\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "#from utils import one_hot_embedding\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit\n",
    "import albumentations\n",
    "\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "\n",
    "from wtfml.utils import EarlyStopping\n",
    "from wtfml.engine import Engine\n",
    "import pretrainedmodels\n",
    "from pretrainedmodels.models import nasnetamobile\n",
    "import cv2\n",
    "import gc\n",
    "import math\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import gmean\n",
    "from librosa.display import specshow\n",
    "from sklearn.utils import class_weight\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "51Dpvo2EIs2-"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "num_seed=42\n",
    "seed_all(num_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x81-1ToVpEMh",
    "outputId": "478b8aa5-37e9-4f64-c80d-a8f31d19e5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 23 13:05:25 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M60           On   | 000068DF:00:00.0 Off |                  Off |\n",
      "| N/A   32C    P8    14W / 150W |      3MiB /  8129MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDZbdWcfGZA7",
    "outputId": "5192d3a5-a588-481f-a082-371b01b02b4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6hTOg7_BlR6"
   },
   "source": [
    "# Utils and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up4hXqtdi4mE"
   },
   "source": [
    "## general settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "owtw_hGLIojb"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  epochs=40\n",
    "  random_state=42\n",
    "  train_batchsize=4\n",
    "  test_batchsize=4\n",
    "  val_every=5\n",
    "  print_every=20\n",
    "  logdir=\"logs\"\n",
    "  DATASET_PATH=\"audio_files\"\n",
    "  DATASET2_PATH=\"latest_keywords\"\n",
    "  DATASET3_PATH=\"nlp_keywords\"\n",
    "  n_folds=10\n",
    "  test_size=0.1\n",
    "  lr=1.2*1e-4\n",
    "  aftertrain_lr=2*1e-6\n",
    "  min_lr=0.1*1e-4\n",
    "  experiment_id=\"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(Config.experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYM0FspMi6Po"
   },
   "source": [
    "## audio settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CcWj0gfA1FS4"
   },
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    audio_length=3\n",
    "    sr=44100\n",
    "    #sr=44100\n",
    "    fixed_sr=audio_length*sr\n",
    "    #hop_length = 512\n",
    "    #hop_length = 275\n",
    "    hop_length = 276\n",
    "    fmin = 20\n",
    "    fmax = 8000\n",
    "    n_mels = 64\n",
    "    n_mfcc=13\n",
    "    #n_fft = 8192\n",
    "    n_fft = n_mels*20\n",
    "    #n_fft=8000\n",
    "    min_seconds = 0.1\n",
    "    #CROP_SIZE = 247\n",
    "    WRAP_PAD_PROB = 0.5\n",
    "    pad=400\n",
    "    spec_aug_prob=0.8\n",
    "    mixer_prob=0.0\n",
    "    audio_crop_prob=0.5\n",
    "    height=228\n",
    "    width=400\n",
    "    duration=3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofe5D8qVC8B1"
   },
   "source": [
    "## plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rSw0i21SCRQ5"
   },
   "outputs": [],
   "source": [
    "def plot_signal(signals):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    axes.set_title(\"sig\")\n",
    "    axes.plot(list(signals))\n",
    "    \n",
    "    \n",
    "def plot_signals(signals):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Time Series', size=16)\n",
    "    i = 0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(signals.keys())[i])\n",
    "            axes[x,y].plot(list(signals.values())[i])\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i += 1\n",
    "\n",
    "def plot_fft(fft):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Fourier Transforms', size=16)\n",
    "    i = 0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            data = list(fft.values())[i]\n",
    "            Y, freq = data[0], data[1]\n",
    "            axes[x,y].set_title(list(fft.keys())[i])\n",
    "            axes[x,y].plot(freq, Y)\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i += 1\n",
    "\n",
    "def plot_fbank(fbank):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Filter Bank Coefficients', size=16)\n",
    "    i = 0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(fbank.keys())[i])\n",
    "            axes[x,y].imshow(list(fbank.values())[i],\n",
    "                    cmap='hot', interpolation='nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i += 1\n",
    "\n",
    "def plot_mfccs(mfccs):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, sharex=False,\n",
    "                             sharey=True, figsize=(20,5))\n",
    "    \n",
    "    axes.set_title(\"mfcc\")\n",
    "    \n",
    "    specshow(mfccs,x_axis='time',y_axis='mel', \n",
    "                             sr=AudioConfig.sr, hop_length=AudioConfig.hop_length,\n",
    "                            fmin=AudioConfig.fmin, fmax=AudioConfig.fmax)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    \n",
    "    plt.show()\n",
    "def get_plot_mfccs(mfccs):\n",
    "    \n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    \n",
    "    specshow(mfccs,x_axis=\"time\",y_axis=\"mel\", \n",
    "                             sr=AudioConfig.sr,hop_length=AudioConfig.hop_length,\n",
    "                            fmin=AudioConfig.fmin, fmax=AudioConfig.fmax)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    image=io.BytesIO()\n",
    "    fig.savefig(image,bbox_inches='tight',pad_inches=0.0)\n",
    "    img=np.frombuffer(image.getvalue(), dtype='uint8')\n",
    "    img = cv2.imdecode(img,cv2.IMREAD_COLOR)\n",
    "    image.close()\n",
    "    plt.close()\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def plot_class_dist(X):\n",
    "    class_dis=X.groupby(\"label\")[\"fn\"].count()\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set_title('Class Distribution', y=1.08)\n",
    "    ax.pie(class_dis, labels=class_dis.index, autopct='%1.1f%%',\n",
    "          shadow=False, startangle=90)\n",
    "    ax.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_melspectrogram(mels, title='Log-frequency power spectrogram'):\n",
    "    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n",
    "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
    "                            fmin=conf.fmin, fmax=conf.fmax)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc_Ouje2gWOW"
   },
   "source": [
    "## util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PXrn6BrlB4tb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read audio from a path\n",
    "\"\"\"\n",
    "\n",
    "def read_wav(filepath):\n",
    "  \n",
    "  sample_rate, samples = wavfile.read(filepath)\n",
    "  return sample_rate,np.array(samples)\n",
    "\n",
    "\"\"\"\n",
    "Listen to audio sample\n",
    "\"\"\"\n",
    "def listen(samples,sample_rate):\n",
    "  return ipd.Audio(samples, rate=sample_rate)\n",
    "\n",
    "\"\"\"\n",
    "Wav loader for DatasetFolder using librosa\n",
    "\"\"\"\n",
    "def wav_loader(path,sr=AudioConfig.sr,fixed_sr=AudioConfig.fixed_sr):\n",
    "    sample,sr=librosa.load(path,sr=sr)\n",
    "    result=torch.zeros(1,fixed_sr)\n",
    "    length=min(fixed_sr,len(sample))\n",
    "    result[0,:length]=torch.tensor(sample[:length])\n",
    "    return (result,sr)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Wav loader for DatasetFolder using torchaudio\n",
    "\"\"\"\n",
    "\n",
    "def torch_wav_loader(path,sr=AudioConfig.sr,fixed_sr=AudioConfig.fixed_sr):\n",
    "    sample,sr=torchaudio.load_wav(path)\n",
    "    result=torch.zeros(1,fixed_sr)\n",
    "    length=min(fixed_sr,sample.shape[1])\n",
    "    result[0,:length]=sample[0,:length]\n",
    "    return (result,sr)\n",
    "\n",
    "\"\"\"\n",
    "accuracy measure\n",
    "\"\"\"\n",
    "def accuracy(predictions,real):\n",
    "    return (predictions==real).sum()*100/len(predictions)\n",
    "\n",
    "def enveloppe(sig,sr,threshhold):\n",
    "  mask=[]\n",
    "  sig=pd.Series(sig).apply(np.abs)\n",
    "  sig_mean=sig.rolling(window=int(sr/10),min_periods=1,center=True).mean()\n",
    "  for mean in sig_mean:\n",
    "    if mean>threshhold:\n",
    "      mask.append(True)\n",
    "    else:  mask.append(False)\n",
    "  return mask\n",
    "\n",
    "\n",
    "def read_audio(file_path,top_db=60):\n",
    "    min_samples = int(AudioConfig.min_seconds * AudioConfig.sr)\n",
    "    y, sr = librosa.load(file_path, sr=AudioConfig.sr)\n",
    "    \n",
    "    trim_y, trim_idx = librosa.effects.trim(y,top_db=top_db,frame_length=AudioConfig.n_fft, hop_length=AudioConfig.hop_length)  # trim, top_db=default(60)\n",
    "\n",
    "    if len(trim_y) < min_samples:\n",
    "        center = (trim_idx[1] - trim_idx[0]) // 2\n",
    "        left_idx = max(0, center - min_samples // 2)\n",
    "        right_idx = min(len(y), center + min_samples // 2)\n",
    "        trim_y = y[left_idx:right_idx]\n",
    "\n",
    "        if len(trim_y) < min_samples:\n",
    "            padding = min_samples - len(trim_y)\n",
    "            offset = padding // 2\n",
    "            trim_y = np.pad(trim_y, (offset, padding - offset), 'constant')\n",
    "\n",
    "    \n",
    "    return trim_y\n",
    "\n",
    "def test_top_db(filepath,top_db,print_mask=True):\n",
    "  sample=read_audio(filepath)\n",
    "  plot_signal(sample)\n",
    "  sample_test=read_audio(filepath,top_db=top_db)\n",
    "  plot_signal(sample_test)\n",
    "  if(print_mask):  return listen(sample_test,AudioConfig.sr)\n",
    "  else : return listen(sample,AudioConfig.sr)\n",
    "\n",
    "\n",
    "def audio_to_melspectrogram(audio):\n",
    "    \n",
    "    spectrogram = librosa.feature.melspectrogram(audio,\n",
    "                                                 sr=AudioConfig.sr,\n",
    "                                                 n_mels=AudioConfig.n_mels,\n",
    "                                                 n_fft=AudioConfig.n_fft,\n",
    "                                                 hop_length=AudioConfig.hop_length,\n",
    "                                                 fmin=AudioConfig.fmin,\n",
    "                                                 fmax=AudioConfig.fmax,\n",
    "                                                 power=2\n",
    "                                                 )\n",
    "    spectrogram = librosa.power_to_db(spectrogram,ref=np.max)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram\n",
    "\n",
    "def read_as_melspectrogram(file_path,time_stretch=1.0, pitch_shift=0.0,\n",
    "                           debug_display=False):\n",
    "    x = read_audio(file_path)\n",
    "    if time_stretch != 1.0:\n",
    "        x = librosa.effects.time_stretch(x, time_stretch)\n",
    "\n",
    "    if pitch_shift != 0.0:\n",
    "        librosa.effects.pitch_shift(x, config.sampling_rate, n_steps=pitch_shift)\n",
    "\n",
    "    mels = audio_to_melspectrogram(x)\n",
    "    if debug_display:\n",
    "        import IPython\n",
    "        IPython.display.display(IPython.display.Audio(x, rate=config.sampling_rate))\n",
    "        show_melspectrogram(mels)\n",
    "    return (mels,AudioConfig.sr)\n",
    "\n",
    "def mix_up(x, y):\n",
    "        x = np.array(x, np.float32)\n",
    "        lam = np.random.beta(1.0, 1.0)\n",
    "        ori_index = np.arange(int(len(x)))\n",
    "        index_array = np.arange(int(len(x)))\n",
    "        np.random.shuffle(index_array)        \n",
    "        \n",
    "        mixed_x = lam * x[ori_index] + (1 - lam) * x[index_array]\n",
    "        mixed_y = lam * y[ori_index] + (1 - lam) * y[index_array]\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "def oversample(dataframe):\n",
    "    X,y=RandomOverSampler(random_state=42).fit_sample(dataframe, dataframe[\"label\"])\n",
    "    return pd.DataFrame(X,columns=dataframe.columns).reset_index(drop=True)\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return torch.eye(num_classes, dtype=float)[y]\n",
    "\n",
    "\"\"\"\n",
    "function to test the enveloppe\n",
    "\"\"\"\n",
    "def test_enveloppe(filepath,thresh=0.0005,print_mask=True):\n",
    "  sample=read_audio(filepath)\n",
    "  print(sample.max())\n",
    "  plot_signal(sample)\n",
    "  mask=enveloppe(sample,AudioConfig.sr,thresh)\n",
    "  plot_signal(sample[mask])\n",
    "  if(print_mask):  return listen(sample[mask],AudioConfig.sr)\n",
    "  else : return listen(sample,AudioConfig.sr)\n",
    "\n",
    "def create_csv_dataset_from_path(dataset_path):\n",
    "    classes=[classe for classe in os.listdir(dataset_path)]\n",
    "    class_to_idx={classe:idx for idx,classe in enumerate(classes)}\n",
    "    idx_to_class={idx:classe for idx,classe in enumerate(classes)}\n",
    "    \n",
    "    path=[]\n",
    "    target=[]\n",
    "    for classe in classes:\n",
    "        class_path=os.path.join(dataset_path,classe)\n",
    "        for sample in os.listdir(class_path):\n",
    "            path.append(os.path.join(class_path,sample))\n",
    "            target.append(classe)\n",
    "    \n",
    "    dataset=pd.DataFrame(data={\"fn\":path,\"label\":target})\n",
    "    dataset = dataset.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    return dataset,classes\n",
    "\n",
    "def onset_test(path):\n",
    "    y=read_audio(path)\n",
    "    times = librosa.times_like(audio_to_melspectrogram(y))\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=AudioConfig.sr,\n",
    "                                         aggregate=np.median,\n",
    "                                         n_fft=AudioConfig.n_fft,\n",
    "                                        hop_length=AudioConfig.hop_length,\n",
    "                                         fmax=8000, n_mels=160)\n",
    "    print(onset_env.argmax())\n",
    "    print(onset_env.shape)\n",
    "    plt.plot(times, 1 + onset_env / onset_env.max(), alpha=0.8,\n",
    "           label='Median (custom mel)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "672QOn7droRK"
   },
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6suJsK7f8Yk"
   },
   "source": [
    "### wav trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ywfq9BNegSIX"
   },
   "outputs": [],
   "source": [
    "class ChangeAmplitude(object):\n",
    "    \"\"\"Changes amplitude of an audio randomly.\"\"\"\n",
    "\n",
    "    def __init__(self, amplitude_range=(0.7, 1.1)):\n",
    "        self.amplitude_range = amplitude_range\n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "        \n",
    "\n",
    "        image = image * random.uniform(*self.amplitude_range)\n",
    "        return data\n",
    "\n",
    "class ChangeSpeedAndPitchAudio(object):\n",
    "    \"\"\"Change the speed of an audio. This transform also changes the pitch of the audio.\"\"\"\n",
    "\n",
    "    def __init__(self, max_scale=0.2):\n",
    "        self.max_scale = max_scale\n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "        \n",
    "\n",
    "        samples = image\n",
    "        sample_rate = AudioConfig.sr\n",
    "        scale = random.uniform(-self.max_scale, self.max_scale)\n",
    "        speed_fac = 1.0  / (1 + scale)\n",
    "        image = np.interp(np.arange(0, len(samples), speed_fac), np.arange(0,len(samples)), samples).astype(np.float32)\n",
    "        return image\n",
    "\n",
    "class StretchAudio(object):\n",
    "    \"\"\"Stretches an audio randomly.\"\"\"\n",
    "\n",
    "    def __init__(self, max_scale=0.2):\n",
    "        self.max_scale = max_scale\n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "        \n",
    "\n",
    "        scale = random.uniform(-self.max_scale, self.max_scale)\n",
    "        image = librosa.effects.time_stretch(image, 1+scale)\n",
    "        return image\n",
    "class TimeshiftAudio(object):\n",
    "    \"\"\"Shifts an audio randomly.\"\"\"\n",
    "\n",
    "    def __init__(self, max_shift_seconds=0.2):\n",
    "        self.max_shift_seconds = max_shift_seconds\n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "\n",
    "        samples = image\n",
    "        sample_rate = AudioConfig.sr\n",
    "        max_shift = (sample_rate * self.max_shift_seconds)\n",
    "        shift = random.randint(-max_shift, max_shift)\n",
    "        a = -min(0, shift)\n",
    "        b = max(0, shift)\n",
    "        samples = np.pad(samples, (a, b), \"constant\")\n",
    "        image = samples[:len(samples) - a] if a else samples[b:]\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2aTvelHZgXmJ"
   },
   "outputs": [],
   "source": [
    "def gauss_noise(k,sig):\n",
    "     return np.random.normal(scale=k*np.max(sig), size=len(sig))\n",
    "\n",
    "def gn(samples,k=2e-2):\n",
    "    noise_g = gauss_noise(k,samples)\n",
    "    return samples+noise_g\n",
    "\n",
    "class GN(object):\n",
    "    \"\"\"Adds a random background noise.\"\"\"\n",
    "    def __init__(self, ):\n",
    "        None\n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "        k=np.random.uniform(low=8e-3, high=4e-2, size=None)\n",
    "        sample=self.gn(image,k=k)\n",
    "        return sample\n",
    "\n",
    "\"\"\"\n",
    "testing gaussian noise\n",
    "\"\"\"\n",
    "def test_transform(filepath,k=2e-2,print_mask=False):\n",
    "  sr=AudioConfig.sr\n",
    "  sample=read_audio(filepath)\n",
    "  plot_signal(sample)\n",
    "  \n",
    "  sample2=gn(sample,k=k)\n",
    "  plot_signal(sample2)\n",
    "  if(print_mask):  return listen(sample2,sr)\n",
    "  else : return listen(sample,sr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Gh-yj2Xf-x3"
   },
   "source": [
    "### spect transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-3XhMhrcg0D0"
   },
   "outputs": [],
   "source": [
    "class ToMfcc(object):\n",
    "    def __call__(self,image,**kwargs):\n",
    "        return audio_to_melspectrogram(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ssDjVP1RyStS"
   },
   "outputs": [],
   "source": [
    "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6,**kwargs):\n",
    "    X=X.transpose(1, 0, 2)\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "  \n",
    "\n",
    "class ToColor:\n",
    "    def __init__(self,\n",
    "                  mean=None,\n",
    "                  std=None):\n",
    "        self.mean=mean\n",
    "        self.std = std\n",
    "        \n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "        return mono_to_color(image,\n",
    "                            self.mean,\n",
    "                            self.std,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8Uf34VjlyUYW"
   },
   "outputs": [],
   "source": [
    "class AudioCrop:\n",
    "    def __init__(self,percentage=0.75):\n",
    "        self.percentage=percentage\n",
    "\n",
    "    def __call__(self,image,**kwargs):\n",
    "        perc=np.random.random()*(1-self.percentage)+self.percentage\n",
    "        return albumentations.RandomCrop(image.shape[0],int(image.shape[1]*perc),p=1)(image=image)[\"image\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zl8-3gOxod7c"
   },
   "outputs": [],
   "source": [
    "class Onset:\n",
    "    def __init__(self,size):\n",
    "        self.size=size\n",
    "\n",
    "    def __call__(self,image,**kwargs):\n",
    "        onset_env = librosa.onset.onset_strength(S=image)\n",
    "        argmax=onset_env.argmax()\n",
    "        return albumentations.Crop(x_min=argmax-self.size//2, y_min=0, x_max=argmax+self.size//2, y_max=AudioConfig.n_mels,p=1)(image=image)[\"image\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uz4KGA4aNIt4"
   },
   "outputs": [],
   "source": [
    "class PadToSize:\n",
    "    def __init__(self, size, mode='constant'):\n",
    "        #assert mode in ['constant', 'wrap']\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "        if image.shape[1] < self.size:\n",
    "            padding = self.size - image.shape[1]\n",
    "            offset = padding // 2\n",
    "            pad_width = ((0, 0), (offset, padding - offset))\n",
    "            #pad_width = ((0, 0), (0, padding ))\n",
    "            if self.mode == 'constant':\n",
    "                \n",
    "                #image = np.pad(image, pad_width,'constant', constant_values=image.min())\n",
    "                image = np.pad(image, pad_width,'constant', constant_values=0)\n",
    "            else:\n",
    "                image = np.pad(image, pad_width, 'wrap')\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "h85CAKDO8Jrx"
   },
   "outputs": [],
   "source": [
    "class AudioPad:\n",
    "    def __init__(self,percentage=0.10, mode='constant'):\n",
    "        self.percentage=percentage\n",
    "        self.mode=mode\n",
    "    def __call__(self,image,**kwargs):\n",
    "        return PadToSize(int(image.shape[1]*(self.percentage+1)),self.mode)(image=image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lFK_xL-vPr3x"
   },
   "outputs": [],
   "source": [
    "class ImageStack:\n",
    "    def __call__(self, image,**kwargs):\n",
    "        delta = librosa.feature.delta(image)\n",
    "        accelerate = librosa.feature.delta(image, order=2)\n",
    "        image = np.stack([image, delta, accelerate], axis=-1)\n",
    "        image = image.astype(np.float32)\n",
    "        return image\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "G99PGSMw_A3J"
   },
   "outputs": [],
   "source": [
    "def spec_augment(spec: np.ndarray,\n",
    "                 num_mask=2,\n",
    "                 freq_masking=0.15,\n",
    "                 time_masking=0.20,\n",
    "                 value=0):\n",
    "    spec = spec.copy()\n",
    "    num_mask = random.randint(1, num_mask)\n",
    "    for i in range(num_mask):\n",
    "        all_freqs_num, all_frames_num  = spec.shape\n",
    "        freq_percentage = random.uniform(0.0, freq_masking)\n",
    "\n",
    "        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n",
    "        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n",
    "        f0 = int(f0)\n",
    "        spec[f0:f0 + num_freqs_to_mask, :] = value\n",
    "\n",
    "        time_percentage = random.uniform(0.0, time_masking)\n",
    "\n",
    "        num_frames_to_mask = int(time_percentage * all_frames_num)\n",
    "        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n",
    "        t0 = int(t0)\n",
    "        spec[:, t0:t0 + num_frames_to_mask] = value\n",
    "    return spec\n",
    "\n",
    "\n",
    "class SpecAugment:\n",
    "    def __init__(self,\n",
    "                 num_mask=2,\n",
    "                 freq_masking=0.15,\n",
    "                 time_masking=0.20):\n",
    "        self.num_mask = num_mask\n",
    "        self.freq_masking = freq_masking\n",
    "        self.time_masking = time_masking\n",
    "\n",
    "    def __call__(self, image,**kwargs):\n",
    "        return spec_augment(image,\n",
    "                            self.num_mask,\n",
    "                            self.freq_masking,\n",
    "                            self.time_masking,\n",
    "                            image.min())\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh6HCuLTht4F"
   },
   "source": [
    "### Get transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "p3IpGNumrqfN"
   },
   "outputs": [],
   "source": [
    "def get_transforms(train, height,width,\n",
    "                   wrap_pad_prob=0.5,\n",
    "                   resize_scale=(1, 0.8),\n",
    "                   resize_ratio=(1, 2.4),\n",
    "                   resize_prob=0.4,\n",
    "                   spec_num_mask=2,\n",
    "                   spec_freq_masking=0.15,\n",
    "                   spec_time_masking=0.20,\n",
    "                   spec_prob=0.5):\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    if train:\n",
    "      \n",
    "        transforms = albumentations.Compose([\n",
    "            \n",
    "            \n",
    "            \n",
    "            albumentations.OneOf([albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"constant\"),p=0.5),\n",
    "                                  albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=0.5),\n",
    "                                  #albumentations.Resize(height,width,p=0.6),\n",
    "                                  #albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio,p=0.3),\n",
    "                                  #albumentations.RandomResizedCrop(height,width,scale=(1,0.9), ratio=(1,2.0),p=0.2),     \n",
    "                                    ],p=1),\n",
    "            \n",
    "            albumentations.Lambda(AudioCrop(percentage=0.9), p=AudioConfig.audio_crop_prob),\n",
    "            #albumentations.RandomCrop(height,width,p=1),\n",
    "            \n",
    "            #albumentations.OneOf([albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio),\n",
    "            #                      albumentations.RandomResizedCrop(height,width,scale=(1,0.9), ratio=(1,2.0)), ],p=resize_prob),\n",
    "            #albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio,p=resize_prob),\n",
    "\n",
    "            \n",
    "            \n",
    "            #albumentations.Compose([albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=1),\n",
    "            #                        albumentations.RandomCrop(AudioConfig.n_mels,width,p=1),],p=1),\n",
    "            albumentations.RandomResizedCrop(height,width,scale=resize_scale, ratio=resize_ratio,p=0.0),\n",
    "            #albumentations.CenterCrop(AudioConfig.n_mels,width,p=1),\n",
    "            #albumentations.RandomCrop(AudioConfig.n_mels,width,p=1),\n",
    "            \n",
    "            #albumentations.Crop(x_min=0, y_min=0, x_max=width, y_max=AudioConfig.n_mels,p=1),\n",
    "            albumentations.Resize(height,width,p=1),\n",
    "            albumentations.OneOf([albumentations.Lambda(SpecAugment(num_mask=2,freq_masking=0.10,time_masking=0.16)),\n",
    "                                  #albumentations.Lambda(SpecAugment()) ,\n",
    "                                  ],p=AudioConfig.spec_aug_prob),\n",
    "                                  \n",
    "            \n",
    "            albumentations.Lambda(ImageStack(),p=1),\n",
    "            albumentations.Lambda(ToColor(),p=1),\n",
    "            albumentations.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "            albumentations.pytorch.transforms.ToTensor(),\n",
    "            \n",
    "            \n",
    "        ])\n",
    "    else:\n",
    "        transforms = albumentations.Compose([\n",
    "            albumentations.Lambda(PadToSize(AudioConfig.pad,mode=\"wrap\"),p=1),\n",
    "            #albumentations.Crop(x_min=0, y_min=0, x_max=width, y_max=AudioConfig.n_mels,p=1),\n",
    "            albumentations.CenterCrop(AudioConfig.n_mels,width,p=1),\n",
    "            albumentations.Resize(height,width,p=1),\n",
    "            albumentations.Lambda(ImageStack(),p=1),\n",
    "            albumentations.Lambda(ToColor(),p=1),\n",
    "            albumentations.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "            albumentations.pytorch.transforms.ToTensor(),\n",
    "            \n",
    "        ])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jziAsIAj5umN"
   },
   "source": [
    "## Mixers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "pwNhBxJq5v0x"
   },
   "outputs": [],
   "source": [
    "def get_random_sample(dataset):\n",
    "    rnd_idx = random.randint(0, len(dataset) - 1)\n",
    "    rnd_audio,rnd_target=dataset.tensor_dict[df.loc[rnd_idx,dataset.path_col]]\n",
    "    rnd_target=dataset.class_to_idx[rnd_target]\n",
    "    \n",
    "    rnd_audio = dataset.transform(image=rnd_audio[0])[\"image\"]\n",
    "    rnd_target=dataset.target_transform(rnd_target,num_classes=len(dataset.classes))\n",
    "    return rnd_audio, rnd_target\n",
    "\n",
    "class AddMixer:\n",
    "    def __init__(self, alpha_dist='uniform'):\n",
    "        assert alpha_dist in ['uniform', 'beta']\n",
    "        self.alpha_dist = alpha_dist\n",
    "\n",
    "    def sample_alpha(self):\n",
    "        if self.alpha_dist == 'uniform':\n",
    "            return random.uniform(0, 0.5)\n",
    "        elif self.alpha_dist == 'beta':\n",
    "            return np.random.beta(0.4, 0.4)\n",
    "\n",
    "    def __call__(self, dataset, image, target):\n",
    "        rnd_image, rnd_target = get_random_sample(dataset)\n",
    "        alpha = self.sample_alpha()\n",
    "        image = (1 - alpha) * image + alpha * rnd_image\n",
    "        target = (1 - alpha) * target + alpha * rnd_target\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class SigmoidConcatMixer:\n",
    "    def __init__(self, sigmoid_range=(3, 12)):\n",
    "        self.sigmoid_range = sigmoid_range\n",
    "\n",
    "    def sample_mask(self, size):\n",
    "        x_radius = random.randint(*self.sigmoid_range)\n",
    "\n",
    "        step = (x_radius * 2) / size[1]\n",
    "        x = np.arange(-x_radius, x_radius, step=step)\n",
    "        y = torch.sigmoid(torch.from_numpy(x)).numpy()\n",
    "        mix_mask = np.tile(y, (size[0], 1))\n",
    "        return torch.from_numpy(mix_mask.astype(np.float32))\n",
    "\n",
    "    def __call__(self, dataset, image, target):\n",
    "        rnd_image, rnd_target = get_random_sample(dataset)\n",
    "\n",
    "        mix_mask = self.sample_mask(image.shape[-2:])\n",
    "        rnd_mix_mask = 1 - mix_mask\n",
    "\n",
    "        image = mix_mask * image + rnd_mix_mask * rnd_image\n",
    "        target = target + rnd_target\n",
    "        target = np.clip(target, 0.0, 1.0)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomMixer:\n",
    "    def __init__(self, mixers, p=None):\n",
    "        self.mixers = mixers\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, dataset, image, target):\n",
    "        mixer = np.random.choice(self.mixers, p=self.p)\n",
    "        image, target = mixer(dataset, image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class UseMixerWithProb:\n",
    "    def __init__(self, mixer, prob=.5):\n",
    "        self.mixer = mixer\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, dataset, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            return self.mixer(dataset, image, target)\n",
    "            print(image.shape,target.shape)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DinrkdkYBhNf"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dYmN4kE-JAyS"
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"Train.csv\")\n",
    "submission=pd.read_csv(\"SampleSubmission.csv\")\n",
    "submission[\"label\"]=\"akawuka\"\n",
    "df2,_=create_csv_dataset_from_path(Config.DATASET2_PATH)\n",
    "df3,_=create_csv_dataset_from_path(Config.DATASET3_PATH)\n",
    "df=pd.concat([df1,df2,df3],ignore_index=True).reset_index(drop=True)\n",
    "#df=df1\n",
    "df_all=pd.concat([df1,df2,df3],ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36rXFfKSGR5_",
    "outputId": "d00cecde-2648-444b-9d83-21ffa19f0768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the dataset :  2126\n",
      "train1 shape :  (1109, 2)\n",
      "train2 shape :  (1740, 2)\n",
      "train3 shape :  (1860, 2)\n",
      "train all shape :  (4709, 2)\n",
      "test shape :  (1017, 195)\n"
     ]
    }
   ],
   "source": [
    "print(\"Files in the dataset : \",len(os.listdir(Config.DATASET_PATH)))\n",
    "print(\"train1 shape : \",df1.shape)\n",
    "print(\"train2 shape : \",df2.shape)\n",
    "print(\"train3 shape : \",df3.shape)\n",
    "print(\"train all shape : \",df.shape)\n",
    "print(\"test shape : \",submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lerCPU6BGZ4C"
   },
   "outputs": [],
   "source": [
    "classes=df[\"label\"].unique()\n",
    "class_to_idx={classe:idx for idx,classe in enumerate(classes)}\n",
    "idx_to_class={idx:classe for idx,classe in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNOiHN30vu_G"
   },
   "source": [
    "# Dataset definition and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dcBYAbEwLuVS"
   },
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, df, loader,classes=None, transform=None,\n",
    "                 target_transform=None,device=torch.device(\"cpu\")):\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        self.df=df.reset_index(drop=True)\n",
    "        self.loader=loader\n",
    "        self.transform=transform\n",
    "        \n",
    "        self.target_transform=target_transform\n",
    "        self.device=device\n",
    "        self.loaded=False\n",
    "        self.loaded_samples=[]\n",
    "        self.path_col=\"fn\"\n",
    "        self.target_col=\"label\"\n",
    "        if classes is None:\n",
    "            self.classes=df[self.target_col].unique()\n",
    "        else :\n",
    "            self.classes=classes\n",
    "        \n",
    "        self.class_to_idx={classe:idx for idx,classe in enumerate(self.classes)}\n",
    "        self.idx_to_class={idx:classe for idx,classe in enumerate(self.classes)}\n",
    "        \n",
    "    \n",
    "    def load_data(self):\n",
    "        self.loaded_samples=[]\n",
    "        for ind in tqdm(range(len(self.df)),0):\n",
    "            path=self.df.loc[ind,self.path_col]\n",
    "            target=self.df.loc[ind,self.target_col]\n",
    "            sample = self.loader(path)\n",
    "            self.loaded_samples.append([sample,target])\n",
    "        self.loaded=True\n",
    "        \n",
    "    def save_tensor(self,path):\n",
    "        assert self.loaded==True\n",
    "        torch.save(self.loaded_samples,path)\n",
    "    def load_tensor(self,path):\n",
    "        self.loaded_samples=torch.load(path)\n",
    "        self.loaded=True\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            dict {\"audio\" \"sample_rate\" \"target\"}\n",
    "        \"\"\"\n",
    "        if self.loaded:\n",
    "            sample, target = self.loaded_samples[index]\n",
    "        \n",
    "        else:    \n",
    "            path=self.df.loc[index,self.path_col]\n",
    "            target=self.df.loc[index,self.target_col]\n",
    "            sample = self.loader(path)\n",
    "\n",
    "        audio=sample[0]\n",
    "        sample_rate=sample[1]\n",
    "        target=self.class_to_idx[target]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            audio = self.transform(image=audio)[\"image\"]\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        return {\"audio\":audio,\"sample_rate\":sample_rate ,\"target\":target}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XH4xyFDV_yKa"
   },
   "outputs": [],
   "source": [
    "class PathDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tensor_dict,classes=None, transform=None,\n",
    "                  target_transform=None,mixer=None,device=torch.device(\"cpu\")):\n",
    "          super(PathDataset, self).__init__()\n",
    "          \n",
    "          self.df=df.reset_index(drop=True)\n",
    "          self.tensor_dict=tensor_dict\n",
    "          self.transform=transform\n",
    "          self.mixer=mixer\n",
    "          self.target_transform=target_transform\n",
    "          self.device=device\n",
    "          self.path_col=\"fn\"\n",
    "          self.target_col=\"label\"\n",
    "          if classes is None:\n",
    "              self.classes=df[self.target_col].unique()\n",
    "          else :\n",
    "              self.classes=classes\n",
    "          \n",
    "          self.class_to_idx={classe:idx for idx,classe in enumerate(self.classes)}\n",
    "          self.idx_to_class={idx:classe for idx,classe in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "          \"\"\"\n",
    "          Args:\n",
    "              index (int): Index\n",
    "\n",
    "          Returns:\n",
    "              dict {\"audio\" \"sample_rate\" \"target\"}\n",
    "          \"\"\"\n",
    "          \n",
    "          path=self.df.loc[index,self.path_col]\n",
    "          #target=self.df.loc[index,self.target_col]\n",
    "          sample,target = self.tensor_dict[path]\n",
    "          audio=sample[0]\n",
    "          sample_rate=sample[1]\n",
    "          target=class_to_idx[target]\n",
    "          \n",
    "          if self.transform is not None:\n",
    "              audio = self.transform(image=audio)[\"image\"]\n",
    "\n",
    "          if self.target_transform is not None:\n",
    "              target = self.target_transform(target,num_classes=len(self.classes))\n",
    "\n",
    "          \n",
    "          if self.mixer is not None:\n",
    "              audio, target = self.mixer(self, audio, target)\n",
    "\n",
    "          \n",
    "          return {\"audio\":audio,\"sample_rate\":sample_rate ,\"target\":target}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "UoL2RSWjB1ho"
   },
   "outputs": [],
   "source": [
    "train_transform = get_transforms(train=True,height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ClD_PHK_63YZ"
   },
   "outputs": [],
   "source": [
    "amixer = RandomMixer([\n",
    "        SigmoidConcatMixer(sigmoid_range=(3, 12)),\n",
    "        AddMixer(alpha_dist='uniform')\n",
    "    ], p=[0.6, 0.4])\n",
    "amixer = UseMixerWithProb(amixer, prob=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "N4l0T4PnPrKa"
   },
   "outputs": [],
   "source": [
    "audio_set=CSVDataset(df_all,read_as_melspectrogram,classes=classes,transform=train_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Hrha5HiNmwYI"
   },
   "outputs": [],
   "source": [
    "submission_set=CSVDataset(submission,read_as_melspectrogram,classes=classes,transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "57fe87ff800d44e4936053202efb40b6",
      "5d68d7ce75164c7c99254c825e307a54",
      "dd6a8412965a455a89b1146769f04c02",
      "c383758e25c94d35912f136f36c087dd",
      "2cc4eb243fff4e529d829bcbbfa19f63",
      "82e56581231e445d9a4475f0ff496d01",
      "08bda76b602f4c56a08fca6217e5d1f3",
      "7137008e45c84fb2a2ce8a0ece72cfdb"
     ]
    },
    "id": "Z2-o95eNKVUZ",
    "outputId": "fc6d3427-7d36-4724-ceee-c098f95826a1"
   },
   "outputs": [],
   "source": [
    "#audio_set.load_data()\n",
    "audio_set.load_tensor(\"dataset.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "VnzdXCN9B7NL"
   },
   "outputs": [],
   "source": [
    "tensor_dict={path:audio_set.loaded_samples[idx] for idx,path in enumerate(df_all[audio_set.path_col])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WzXOWW_0Cz-i"
   },
   "outputs": [],
   "source": [
    "loaded_set=PathDataset(df_all,tensor_dict,classes=classes,transform=train_transform,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jNUNeYcD7R-",
    "outputId": "3e7d73c5-d275-43bd-b422-df16531d2982"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 400, 228])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_set.__getitem__(457)[\"audio\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "e7519b62f0574195b9f85f1b77342ef7",
      "17a0a356cd1c4c52b8d2ac36a5f0cd92",
      "2015a3677fdb456a81ef9d0dd4c3aa03",
      "a63e2652392749a8a3cc1784cccd0f0a",
      "fad062f953014bac84e12360835164ac",
      "fdcf6a876a5a4bc4a48f2c9923fcbfc4",
      "9432f3eb3cee4a58bbe3da8530e81736",
      "2452ea5f11a84f8199c23401fa7c055f"
     ]
    },
    "id": "JUnnVgyub9jb",
    "outputId": "6e310459-4fb3-49aa-ba87-5862706c3300"
   },
   "outputs": [],
   "source": [
    "#submission_set.load_data()\n",
    "submission_set.load_tensor(\"submission.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Zw_moWkWcDdn"
   },
   "outputs": [],
   "source": [
    "submission_tensor_dict={path:submission_set.loaded_samples[idx] for idx,path in enumerate(submission[submission_set.path_col])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apxsiVm2pad6"
   },
   "source": [
    "# Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Hdpk0JI9sqbJ"
   },
   "outputs": [],
   "source": [
    "df1[\"num_label\"]=df1.label.apply(lambda x:class_to_idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yP4rQaujpZ0K",
    "outputId": "782bd6fa-30e5-4bd6-d449-37671609f7ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=['akawuka' 'banana' 'obulwadde' 'nnyaanya' 'pampu' 'obutunda' 'plantation'\n",
      " 'ensujju' 'okulimibwa' 'mpeke' 'okusaasaana' 'ebigimusa' 'ekikolo' 'farm'\n",
      " 'kisaanyi' 'kikajjo' 'ekisaanyi' 'ndwadde' 'omusiri' 'butterfly'\n",
      " 'munyeera' 'eggobe' 'ebiwojjolo' 'ebisoolisooli' 'namuginga' 'okugimusa'\n",
      " 'maize streak virus' 'ekirime' 'miceere' 'sikungula' 'lumonde'\n",
      " 'okukungula' 'cassava' 'ebirime' 'ebijanjaalo' 'weeding' 'garden'\n",
      " 'drought' 'leaves' 'insect' 'akatungulu' 'seed' 'pepper'\n",
      " 'matooke seedlings' 'harvesting' 'medicine' 'nursery bed' 'mucungwa'\n",
      " 'endwadde' 'pawpaw' 'enkota' 'ensiringanyi' 'kassooli' 'okufuuyira'\n",
      " 'caterpillars' 'ekijanjaalo' 'okukkoola' 'crop' 'okulima' 'endagala'\n",
      " 'kaamulali' 'ennima' 'omuceere' 'micungwa' 'ebisaanyi' 'plant' 'eddagala'\n",
      " 'ennimiro' 'amakoola' 'ebiwuka' 'ekigimusa' 'bibala' 'beans' 'nnimiro'\n",
      " 'ebinyebwa' 'passion fruit' 'Spinach' 'okuzifuuyira' 'ekirwadde'\n",
      " 'nakavundira' 'nfukirira' 'onion' 'ddagala' 'muwogo' 'irrigate'\n",
      " 'akasaanyi' 'ekikajjo' 'emmwanyi' 'ekiwojjolo' 'orange' 'ebibala'\n",
      " 'ebyobulimi' 'ensuku' 'farmer' 'spray' 'obumonde' 'nnasale beedi'\n",
      " 'abalimi' 'okusaasaanya' 'doodo' 'enva endiirwa' 'ebikolo' 'obusaanyi'\n",
      " 'omulimisa' 'muceere' 'ejjobyo' 'ebikajjo' 'omucungwa' 'amappapaali'\n",
      " 'ensigo' 'ebikoola' 'emboga' 'spread' 'akamonde' 'kasaanyi' 'dig'\n",
      " 'ebisooli' 'nnakati' 'obulimi' 'mangoes' 'sweet potatoes' 'akammwanyi'\n",
      " 'vegetables' 'worm' 'amakungula' 'omuyembe' 'harvest' 'olusuku'\n",
      " 'amalagala' 'npk' 'kikolo' 'maize' 'coffee' 'ebijjanjalo'\n",
      " 'irish potatoes' 'ebimera' 'matooke' 'leaf' 'afukirira' 'ensukusa'\n",
      " 'caterpillar' 'sukumawiki' 'suckers' 'amatooke' 'emiyembe' 'endokwa'\n",
      " 'okusimba' 'mulimi' 'farming instructor' 'fertilizer' 'kukungula'\n",
      " 'akatunda' 'omulimi' 'nambaale' 'ebikongoliro' 'sow' 'ground nuts'\n",
      " 'super grow' 'ekimera' 'fruit picking' 'obuwuka' 'okusiga' 'emisiri'\n",
      " 'ekitooke' 'emicungwa' 'pumpkin' 'greens' 'bulimi' 'agriculture'\n",
      " 'okufukirira' 'tomatoes' 'fruit' 'ebitooke' 'rice' 'ebbugga' 'ppaapaali'\n",
      " 'okunnoga' 'obutungulu' 'ennyaanya' 'lusuku' 'insects' 'mango'\n",
      " 'eppapaali' 'Pump' 'maize stalk borer' 'ekibala' 'watermelon' 'ekyeya'\n",
      " 'disease' 'ekikoola' 'faamu' 'cabbages' 'sugarcane'], y=0           akawuka\n",
      "1            banana\n",
      "2         obulwadde\n",
      "3          nnyaanya\n",
      "4             pampu\n",
      "           ...     \n",
      "1104        cassava\n",
      "1105     harvesting\n",
      "1106           farm\n",
      "1107    nakavundira\n",
      "1108    nursery bed\n",
      "Name: label, Length: 1109, dtype: object as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5746114 , 1.4365285 , 0.63845711, 0.95768566, 1.1492228 ,\n",
       "       0.82087343, 0.71826425, 0.95768566, 1.1492228 , 0.95768566,\n",
       "       0.63845711, 0.47884283, 1.4365285 , 0.82087343, 0.71826425,\n",
       "       1.4365285 , 0.63845711, 0.522374  , 1.1492228 , 1.91537133,\n",
       "       0.522374  , 1.1492228 , 0.522374  , 0.82087343, 0.5746114 ,\n",
       "       0.5746114 , 1.4365285 , 1.1492228 , 0.95768566, 0.95768566,\n",
       "       0.95768566, 0.95768566, 1.91537133, 1.1492228 , 0.95768566,\n",
       "       0.71826425, 0.82087343, 0.95768566, 0.82087343, 1.1492228 ,\n",
       "       0.95768566, 0.82087343, 1.4365285 , 1.91537133, 0.82087343,\n",
       "       1.4365285 , 0.82087343, 1.1492228 , 0.522374  , 1.4365285 ,\n",
       "       0.95768566, 0.522374  , 0.95768566, 0.5746114 , 1.4365285 ,\n",
       "       0.95768566, 1.4365285 , 0.71826425, 1.1492228 , 0.95768566,\n",
       "       0.82087343, 1.1492228 , 1.4365285 , 1.1492228 , 0.63845711,\n",
       "       0.82087343, 0.63845711, 1.1492228 , 1.4365285 , 0.522374  ,\n",
       "       0.71826425, 0.95768566, 1.91537133, 1.1492228 , 1.1492228 ,\n",
       "       1.91537133, 1.4365285 , 0.5746114 , 0.71826425, 0.63845711,\n",
       "       1.1492228 , 1.91537133, 0.63845711, 0.95768566, 0.82087343,\n",
       "       0.522374  , 1.1492228 , 0.95768566, 0.71826425, 1.91537133,\n",
       "       1.1492228 , 1.1492228 , 1.4365285 , 0.82087343, 1.4365285 ,\n",
       "       0.95768566, 1.1492228 , 1.4365285 , 0.63845711, 0.95768566,\n",
       "       0.82087343, 1.4365285 , 0.63845711, 1.1492228 , 0.95768566,\n",
       "       0.71826425, 0.95768566, 1.1492228 , 0.95768566, 1.1492228 ,\n",
       "       1.1492228 , 1.1492228 , 1.4365285 , 1.1492228 , 0.5746114 ,\n",
       "       0.82087343, 0.82087343, 0.95768566, 1.4365285 , 1.91537133,\n",
       "       1.91537133, 0.82087343, 1.91537133, 1.91537133, 1.1492228 ,\n",
       "       0.95768566, 0.82087343, 1.1492228 , 0.95768566, 1.4365285 ,\n",
       "       1.4365285 , 1.4365285 , 1.4365285 , 1.1492228 , 1.91537133,\n",
       "       0.95768566, 0.95768566, 0.71826425, 1.1492228 , 0.95768566,\n",
       "       1.1492228 , 1.91537133, 0.95768566, 1.1492228 , 0.95768566,\n",
       "       1.91537133, 1.1492228 , 1.4365285 , 0.82087343, 1.1492228 ,\n",
       "       1.4365285 , 0.95768566, 1.1492228 , 0.82087343, 1.4365285 ,\n",
       "       0.82087343, 1.91537133, 1.4365285 , 1.4365285 , 0.82087343,\n",
       "       0.5746114 , 1.4365285 , 1.4365285 , 1.4365285 , 0.71826425,\n",
       "       1.91537133, 1.4365285 , 1.1492228 , 0.82087343, 1.4365285 ,\n",
       "       1.91537133, 0.82087343, 0.95768566, 1.4365285 , 1.4365285 ,\n",
       "       1.1492228 , 1.4365285 , 0.95768566, 0.95768566, 1.1492228 ,\n",
       "       1.4365285 , 1.91537133, 0.95768566, 0.95768566, 1.91537133,\n",
       "       1.91537133, 1.4365285 , 1.91537133, 1.91537133, 1.4365285 ,\n",
       "       1.91537133, 1.91537133, 1.91537133])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes,\n",
    "                                                 df1.label)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swdduFm2_BZh"
   },
   "source": [
    "# Create folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqgyHUwk_DFS",
    "outputId": "ca6934cb-cf15-43ec-d510-b26f48dd680d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df[\"folds\"]=-1\n",
    "df2[\"folds\"]=-1\n",
    "df3[\"folds\"]=-1\n",
    "\n",
    "kf = StratifiedKFold(n_splits=Config.n_folds, random_state=Config.random_state, shuffle=False)\n",
    "for fold, (_, val_index) in enumerate(kf.split(df,df[\"label\"])):\n",
    "        df.loc[val_index, \"folds\"] = fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfuD3RXRig61"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ppGrcXJEiiat"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,arch,num_classes ,pretrained='imagenet'):\n",
    "        super(Net, self).__init__()\n",
    "        self.base_model = pretrainedmodels.__dict__[\n",
    "            arch\n",
    "        ](pretrained=pretrained)\n",
    "        \n",
    "        \n",
    "        self.prepare = torch.nn.Sequential()\n",
    "        self.prepare.add_module('conv', nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1, stride=1,\n",
    "                                                bias=False))\n",
    "        #self.prepare.add_module('bn', nn.BatchNorm2d(3, eps=0.001, momentum=0.1, affine=True))\n",
    "        if arch==\"dpn98\":\n",
    "            self.l0 = torch.nn.Linear(2688, num_classes)\n",
    "        elif arch==\"se_resnext50_32x4d\" or arch==\"resnet101\" :\n",
    "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
    "        elif arch==\"dpn68\":\n",
    "            self.l0 = torch.nn.Linear(832, num_classes)\n",
    "        elif arch==\"resnet18\":\n",
    "            self.l0 = torch.nn.Linear(512, num_classes)\n",
    "            #self.l0 = torch.nn.Linear(1024, num_classes)\n",
    "\n",
    "        elif arch==\"vgg19\":\n",
    "            self.l0 = torch.nn.Linear(512, num_classes)\n",
    "            #self.l0 = torch.nn.Linear(1024, num_classes)\n",
    "\n",
    "        elif arch==\"se_resnet50\":    \n",
    "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
    "        elif arch==\"resnet50\":    \n",
    "            self.l0 = torch.nn.Linear(2048, num_classes)  \n",
    "        elif arch==\"senet154\":\n",
    "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
    "        elif arch==\"se_resnext101_32x4d\":\n",
    "            self.l0 = torch.nn.Linear(2048, num_classes)\n",
    "        elif arch==\"dpn107\":\n",
    "            self.l0 = torch.nn.Linear(2688, num_classes)\n",
    "        elif arch==\"densenet121\":\n",
    "            self.l0 = torch.nn.Linear(1024, num_classes)\n",
    "            fc_size = self.base_model.last_linear.in_features\n",
    "            #print(\"fc_size : \",fc_size)\n",
    "            self.base_model.last_linear = nn.Sequential(nn.Linear(7168, 193))\n",
    "\n",
    "\n",
    "        else :\n",
    "            self.l0 = torch.nn.Linear(4098, num_classes)\n",
    "    def forward(self, audio, target,sample_rate):\n",
    "        batch_size, _, _, _ = audio.shape\n",
    "        \n",
    "        x=audio\n",
    "        x = self.base_model.features(x)\n",
    "        \n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n",
    "        \n",
    "        out = self.l0(x)\n",
    "        \n",
    "        loss = torch.nn.CrossEntropyLoss()(out, torch.argmax(target, dim=1))\n",
    "        \n",
    "\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hu_LCWchgVr"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "0Pnh8tcz5gVp"
   },
   "outputs": [],
   "source": [
    "model_name=\"dpn68\"\n",
    "#model_name=\"dpn98\"\n",
    "#model_name=\"resnet18\"\n",
    "#model_name=\"densenet121\"\n",
    "pretrained=\"imagenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "7ba7b1c4fbc64003ad9a82d3504afb4d",
      "05bb550af7304811a65569f01d32b18e",
      "c01d10eb78c44dad9eded65a3fa4d0f1",
      "7a4bbca4012e4ff28d2a1dd601aaf897",
      "e0736205c4ab429cb6e08211323199cb",
      "3d5e426de3c44c8594e90c1a574dd916",
      "b64f31b66fef479bbb935bd978babf5d",
      "e2ac166380364eebb018c90da064f140"
     ]
    },
    "id": "QkfdfenPC1kS",
    "outputId": "baf86335-2b64-43fa-fbaa-2ca1b6cacc7b"
   },
   "outputs": [],
   "source": [
    "model = Net(model_name,num_classes=len(classes),pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "r5XRMa6M1WKr"
   },
   "outputs": [],
   "source": [
    "def train(fold):\n",
    "    seed_all(num_seed)\n",
    "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
    "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
    "    \n",
    "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
    "    df_train[\"weights\"]=df_train[\"label\"].apply(lambda x:class_weights[class_to_idx[x]])\n",
    "    \n",
    "    print(\"-------------\",df_train.shape,\"---------------\",df_valid.shape,\"-------------\")\n",
    "    train_transfrom = get_transforms(train=True,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    valid_transfrom = get_transforms(train=False,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    mixer = RandomMixer([\n",
    "        \n",
    "        AddMixer(alpha_dist='uniform')\n",
    "    ], p=[1])\n",
    "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
    "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
    "    )\n",
    "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
    "    \n",
    "    model.to(Config.device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=0, factor=0.6,min_lr=Config.min_lr,verbose=True)\n",
    "    \n",
    "    es = EarlyStopping(patience=8, mode=\"min\")\n",
    "    eng = Engine(model, optimizer, device=Config.device)\n",
    "    for epoch in range(Config.epochs):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n",
    "        \n",
    "        with open('out.txt', 'a') as f:\n",
    "            f.write(f\"Fold = {fold}  Epoch = {epoch}, valid loss = {valid_loss}\\n\")\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "        es(valid_loss, model, model_path=model_path)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "tminX_NX_hia"
   },
   "outputs": [],
   "source": [
    "def after_train(fold):\n",
    "    seed_all(num_seed)\n",
    "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
    "    model_save_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
    "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
    "    \n",
    "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
    "    df_train[\"weights\"]=df_train[\"label\"].apply(lambda x:class_weights[class_to_idx[x]])\n",
    "    \n",
    "    print(\"-------------\",df_train.shape,\"---------------\",df_valid.shape,\"-------------\")\n",
    "    train_transfrom = get_transforms(train=True,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    valid_transfrom = get_transforms(train=False,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    mixer = RandomMixer([\n",
    "        \n",
    "        AddMixer(alpha_dist='uniform')\n",
    "    ], p=[1])\n",
    "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
    "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
    "    )\n",
    "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.aftertrain_lr)\n",
    "    \n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=0.2*1e-6, last_epoch=-1,)\n",
    "    \n",
    "    es = EarlyStopping(patience=6, mode=\"min\")\n",
    "    eng = Engine(model, optimizer,scheduler=scheduler, device=Config.device)\n",
    "    for epoch in range(Config.epochs):\n",
    "        if epoch!=0:\n",
    "            train_loss = eng.train(train_loader)\n",
    "        valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n",
    "        \n",
    "        with open('out.txt', 'a') as f:\n",
    "            f.write(f\"Fold = {fold}  Epoch = {epoch}, valid loss = {valid_loss}\\n\")\n",
    "        \n",
    "        \n",
    "        es(valid_loss, model, model_path=model_save_path)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "xgvg-XQQEf2_"
   },
   "outputs": [],
   "source": [
    "def eval_train(fold):\n",
    "    seed_all(num_seed)\n",
    "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
    "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
    "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_transfrom = get_transforms(train=True,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    valid_transfrom = get_transforms(train=False,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    mixer = RandomMixer([\n",
    "        \n",
    "        AddMixer(alpha_dist='uniform')\n",
    "    ], p=[1])\n",
    "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
    "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
    "    )\n",
    "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
    "    eng = Engine(model, optimizer, device=Config.device)\n",
    "    \n",
    "    train_loss,predictions = eng.evaluate(train_loader, return_predictions=True)\n",
    "    valid_loss,predictions = eng.evaluate(valid_loader, return_predictions=True)\n",
    "    \n",
    "    \n",
    "    print(f\"train loss = {train_loss}, valid loss = {valid_loss} \")\n",
    "    return train_loss,valid_loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "m7fZjwth7AYZ"
   },
   "outputs": [],
   "source": [
    "def predict(fold):\n",
    "    seed_all(num_seed)\n",
    "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
    "    test_transfrom = get_transforms(train=False,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    train_transfrom = get_transforms(train=True,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    \n",
    "    \n",
    "    test_dataset =PathDataset(submission,submission_tensor_dict,classes=classes,transform=test_transfrom,target_transform=to_categorical)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
    "    )\n",
    "\n",
    "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.device)\n",
    "    model.eval()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.lr)\n",
    "    eng = Engine(model, optimizer, device=Config.device)\n",
    "    predictions = eng.predict(test_loader)\n",
    "    predictions=torch.nn.Softmax(dim=1)(torch.cat(predictions))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "hWQPt_MTu5OA"
   },
   "outputs": [],
   "source": [
    "def predict_tta(fold):\n",
    "    seed_all(num_seed)\n",
    "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
    "    test_transfrom = get_transforms(train=False,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    train_transfrom = get_transforms(train=True,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    \n",
    "    \n",
    "    test_dataset =PathDataset(submission,submission_tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
    "    )\n",
    "\n",
    "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.device)\n",
    "    model.eval()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
    "    eng = Engine(model, optimizer, device=Config.device)\n",
    "    all_predictions=[]\n",
    "    for i in range(30):\n",
    "        all_predictions.append(torch.nn.Softmax(dim=1)(torch.cat(eng.predict(test_loader))).numpy())\n",
    "    \n",
    "    predictions=gmean(all_predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "EudD6O6ILrfi"
   },
   "outputs": [],
   "source": [
    "def generate_submission_csv(fold):\n",
    "    seed_all(num_seed)\n",
    "    model_path=os.path.join(Config.experiment_id,f\"model_fold_{fold}.bin\")\n",
    "    df_train = df[df[\"folds\"] != fold].reset_index(drop=True)\n",
    "    df_valid = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_transfrom = get_transforms(train=True,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    valid_transfrom = get_transforms(train=False,\n",
    "                                     height=AudioConfig.height,\n",
    "                                     width=AudioConfig.width,\n",
    "                                     wrap_pad_prob=AudioConfig.WRAP_PAD_PROB)\n",
    "    mixer = RandomMixer([\n",
    "        \n",
    "        AddMixer(alpha_dist='uniform')\n",
    "    ], p=[1])\n",
    "    mixer = UseMixerWithProb(mixer, prob=AudioConfig.mixer_prob)\n",
    "    train_dataset =PathDataset(df_train,tensor_dict,classes=classes,transform=train_transfrom,target_transform=to_categorical,mixer=mixer)\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=Config.train_batchsize, shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    valid_dataset =PathDataset(df_valid,tensor_dict,classes=classes,transform=valid_transfrom,target_transform=to_categorical)\n",
    "    \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=Config.test_batchsize, shuffle=False, num_workers=8\n",
    "    )\n",
    "    model = Net(model_name,num_classes=len(classes),pretrained=pretrained)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(Config.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr)\n",
    "    eng = Engine(model, optimizer, device=Config.device)\n",
    "    \n",
    "    \n",
    "    predictions = eng.predict(valid_loader)\n",
    "    \n",
    "    predictions=torch.nn.Softmax(dim=1)(torch.cat(predictions))\n",
    "    sample = df[df[\"folds\"] == fold].reset_index(drop=True)\n",
    "    \n",
    "    sample.loc[:, classes] = predictions\n",
    "    return sample\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgmL6qprRPS9"
   },
   "source": [
    "11:54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XOGbYimYlsb"
   },
   "source": [
    "### Training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "wEDczcmboOhY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for fold in range(0,Config.n_folds):\n",
    "    print(\"Fold : \",fold)\n",
    "    train(fold)\n",
    "    after_train(fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce57903de3874bf18c9899217b74db0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462a2960475140d0b9721c3b95de72b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.12384458328696091, valid loss = 1.187116670897783 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236cdec74e30498e815d651419c8ea72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d7bb06b09e4847a8ee891228084a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.12307793921582348, valid loss = 0.7459480790243784 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7931ebdfef74e2e8406a90809f857f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43623c1684e4108ad97a4f3805116a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.08145786040942492, valid loss = 0.5414975580874776 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578f24703bde402eae518729dfe6ccdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87842c960e743d3a069dc79a371d343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.05222611909441596, valid loss = 0.49727889978064427 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227e0fe850cc43a29f22434e929c8ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ea195ff9ed4a05acf219d73c2bc0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.06339400710921321, valid loss = 0.42139920602420394 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d172778d1d224470b86c91d0c890331f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c436b55cc9494597b0ab97b127786114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.05416902270058103, valid loss = 0.43164998805316873 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f189f39f1574dd7ac3e680b12cb9eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e70b0cbce140c4a26b7c2927f7c44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.07797903835741618, valid loss = 0.23521054835775 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bd79d8af2140e6aba06cc113f4eb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f107cff31340c09f14b992823523c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.06480202990594235, valid loss = 0.2807040279104682 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea61c24e635408ca094005cb05686c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8182960514b148dca017c93ad6b0ad39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.06779549509014512, valid loss = 0.14814615108554205 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bf1e898f8f48d38f088a92c37de22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64257bc746044daa4b6db8566209195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train loss = 0.05680204783891655, valid loss = 0.1526668972801417 \n"
     ]
    }
   ],
   "source": [
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "for fold in range(Config.n_folds):\n",
    "    train_loss,valid_loss=eval_train(fold)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "lt6dIbaOK4qx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4641618026501558"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "N7aK8tRv1bj0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff27e5bcfe5497b8fe9c427a49961cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbc80ac33404a2dbe6475fe5bd80050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28437e6a960f4e368271d7740306ccfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c3b8dec0ae4538b13fe6dd982df5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a28d10c4d4fa1a59a668cc96fdb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10387626372b42438fd96aa318b21d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ae8f68a4bb4b21b7cf1d07da93bea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b29a5553134bab8f6c9be9f66ad0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30589284319145949d446374ad0a392c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d5bbc663d6480eae612424f3b48d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=255.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p=[]\n",
    "for fold in range(Config.n_folds):\n",
    "    p.append(predict(fold).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "693biZU0ptws"
   },
   "outputs": [],
   "source": [
    "predictions=gmean(p,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sxD_ihkLgCCg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017, 193)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "aZ-fHW9EIC0M"
   },
   "outputs": [],
   "source": [
    "prediction_file=f\"dpn68.csv\"\n",
    "sample = pd.read_csv(\"SampleSubmission.csv\")\n",
    "sample.loc[:, classes] = predictions\n",
    "sample.to_csv(prediction_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "vL-IRKiTgCe2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>maize streak virus</th>\n",
       "      <th>disease</th>\n",
       "      <th>okukkoola</th>\n",
       "      <th>muwogo</th>\n",
       "      <th>mpeke</th>\n",
       "      <th>mucungwa</th>\n",
       "      <th>greens</th>\n",
       "      <th>garden</th>\n",
       "      <th>mango</th>\n",
       "      <th>...</th>\n",
       "      <th>kasaanyi</th>\n",
       "      <th>suckers</th>\n",
       "      <th>insects</th>\n",
       "      <th>fertilizer</th>\n",
       "      <th>nakavundira</th>\n",
       "      <th>ekiwojjolo</th>\n",
       "      <th>akawuka</th>\n",
       "      <th>ddagala</th>\n",
       "      <th>ebiwojjolo</th>\n",
       "      <th>obutungulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_files/00118N3.wav</td>\n",
       "      <td>4.982374e-04</td>\n",
       "      <td>2.936035e-05</td>\n",
       "      <td>1.333825e-05</td>\n",
       "      <td>3.919042e-06</td>\n",
       "      <td>2.633253e-05</td>\n",
       "      <td>8.151552e-06</td>\n",
       "      <td>1.120957e-03</td>\n",
       "      <td>9.313720e-03</td>\n",
       "      <td>2.835080e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.567300e-06</td>\n",
       "      <td>1.674182e-04</td>\n",
       "      <td>7.750516e-05</td>\n",
       "      <td>5.945370e-04</td>\n",
       "      <td>8.618630e-07</td>\n",
       "      <td>9.448920e-07</td>\n",
       "      <td>7.436629e-05</td>\n",
       "      <td>4.465838e-04</td>\n",
       "      <td>2.752912e-07</td>\n",
       "      <td>1.438401e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_files/00P0NMV.wav</td>\n",
       "      <td>4.956156e-10</td>\n",
       "      <td>3.945247e-10</td>\n",
       "      <td>1.653673e-10</td>\n",
       "      <td>4.860184e-10</td>\n",
       "      <td>2.686990e-09</td>\n",
       "      <td>8.620225e-09</td>\n",
       "      <td>2.600239e-10</td>\n",
       "      <td>1.438830e-08</td>\n",
       "      <td>9.735198e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.857995e-09</td>\n",
       "      <td>3.664304e-09</td>\n",
       "      <td>3.635627e-10</td>\n",
       "      <td>7.494829e-08</td>\n",
       "      <td>9.991038e-01</td>\n",
       "      <td>8.150544e-09</td>\n",
       "      <td>3.170091e-07</td>\n",
       "      <td>8.116902e-07</td>\n",
       "      <td>1.520849e-09</td>\n",
       "      <td>8.244088e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_files/01QEEZI.wav</td>\n",
       "      <td>3.727014e-07</td>\n",
       "      <td>2.840558e-06</td>\n",
       "      <td>1.689248e-06</td>\n",
       "      <td>9.056070e-05</td>\n",
       "      <td>1.190358e-06</td>\n",
       "      <td>7.335290e-07</td>\n",
       "      <td>4.384382e-07</td>\n",
       "      <td>7.363206e-07</td>\n",
       "      <td>1.020058e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>8.333769e-07</td>\n",
       "      <td>1.431371e-08</td>\n",
       "      <td>2.882434e-07</td>\n",
       "      <td>2.873111e-07</td>\n",
       "      <td>3.088863e-07</td>\n",
       "      <td>7.111642e-07</td>\n",
       "      <td>4.648003e-08</td>\n",
       "      <td>3.707795e-07</td>\n",
       "      <td>5.742815e-06</td>\n",
       "      <td>9.233232e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_files/037YAED.wav</td>\n",
       "      <td>2.327935e-05</td>\n",
       "      <td>9.565458e-06</td>\n",
       "      <td>7.227104e-08</td>\n",
       "      <td>1.340523e-06</td>\n",
       "      <td>3.220598e-04</td>\n",
       "      <td>3.877383e-06</td>\n",
       "      <td>3.179390e-06</td>\n",
       "      <td>4.161313e-01</td>\n",
       "      <td>1.187128e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.186658e-06</td>\n",
       "      <td>3.707174e-03</td>\n",
       "      <td>8.858315e-05</td>\n",
       "      <td>4.840702e-04</td>\n",
       "      <td>8.907200e-06</td>\n",
       "      <td>1.463914e-06</td>\n",
       "      <td>9.296805e-05</td>\n",
       "      <td>3.652636e-04</td>\n",
       "      <td>1.415184e-06</td>\n",
       "      <td>3.810170e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_files/0382N0Y.wav</td>\n",
       "      <td>7.332573e-08</td>\n",
       "      <td>1.563314e-06</td>\n",
       "      <td>7.388575e-08</td>\n",
       "      <td>4.681138e-07</td>\n",
       "      <td>2.524634e-07</td>\n",
       "      <td>5.709543e-07</td>\n",
       "      <td>7.408599e-08</td>\n",
       "      <td>7.139526e-08</td>\n",
       "      <td>6.322460e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.688224e-07</td>\n",
       "      <td>4.939552e-08</td>\n",
       "      <td>5.181317e-08</td>\n",
       "      <td>1.411171e-07</td>\n",
       "      <td>8.881476e-07</td>\n",
       "      <td>1.860517e-06</td>\n",
       "      <td>2.594567e-06</td>\n",
       "      <td>7.256855e-08</td>\n",
       "      <td>1.065122e-05</td>\n",
       "      <td>1.903768e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        fn  maize streak virus       disease     okukkoola  \\\n",
       "0  audio_files/00118N3.wav        4.982374e-04  2.936035e-05  1.333825e-05   \n",
       "1  audio_files/00P0NMV.wav        4.956156e-10  3.945247e-10  1.653673e-10   \n",
       "2  audio_files/01QEEZI.wav        3.727014e-07  2.840558e-06  1.689248e-06   \n",
       "3  audio_files/037YAED.wav        2.327935e-05  9.565458e-06  7.227104e-08   \n",
       "4  audio_files/0382N0Y.wav        7.332573e-08  1.563314e-06  7.388575e-08   \n",
       "\n",
       "         muwogo         mpeke      mucungwa        greens        garden  \\\n",
       "0  3.919042e-06  2.633253e-05  8.151552e-06  1.120957e-03  9.313720e-03   \n",
       "1  4.860184e-10  2.686990e-09  8.620225e-09  2.600239e-10  1.438830e-08   \n",
       "2  9.056070e-05  1.190358e-06  7.335290e-07  4.384382e-07  7.363206e-07   \n",
       "3  1.340523e-06  3.220598e-04  3.877383e-06  3.179390e-06  4.161313e-01   \n",
       "4  4.681138e-07  2.524634e-07  5.709543e-07  7.408599e-08  7.139526e-08   \n",
       "\n",
       "          mango  ...      kasaanyi       suckers       insects    fertilizer  \\\n",
       "0  2.835080e-03  ...  2.567300e-06  1.674182e-04  7.750516e-05  5.945370e-04   \n",
       "1  9.735198e-10  ...  2.857995e-09  3.664304e-09  3.635627e-10  7.494829e-08   \n",
       "2  1.020058e-07  ...  8.333769e-07  1.431371e-08  2.882434e-07  2.873111e-07   \n",
       "3  1.187128e-05  ...  9.186658e-06  3.707174e-03  8.858315e-05  4.840702e-04   \n",
       "4  6.322460e-08  ...  1.688224e-07  4.939552e-08  5.181317e-08  1.411171e-07   \n",
       "\n",
       "    nakavundira    ekiwojjolo       akawuka       ddagala    ebiwojjolo  \\\n",
       "0  8.618630e-07  9.448920e-07  7.436629e-05  4.465838e-04  2.752912e-07   \n",
       "1  9.991038e-01  8.150544e-09  3.170091e-07  8.116902e-07  1.520849e-09   \n",
       "2  3.088863e-07  7.111642e-07  4.648003e-08  3.707795e-07  5.742815e-06   \n",
       "3  8.907200e-06  1.463914e-06  9.296805e-05  3.652636e-04  1.415184e-06   \n",
       "4  8.881476e-07  1.860517e-06  2.594567e-06  7.256855e-08  1.065122e-05   \n",
       "\n",
       "     obutungulu  \n",
       "0  1.438401e-06  \n",
       "1  8.244088e-09  \n",
       "2  9.233232e-07  \n",
       "3  3.810170e-06  \n",
       "4  1.903768e-07  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "UM_WsXTmiy-Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982855086786987"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.iloc[700,1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "P041nF6hwo_m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9969573616981506"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.iloc[700,1:].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE1O2f9yL5cW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sub-19-11-20_exp1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05bb550af7304811a65569f01d32b18e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08bda76b602f4c56a08fca6217e5d1f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17a0a356cd1c4c52b8d2ac36a5f0cd92": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2015a3677fdb456a81ef9d0dd4c3aa03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdcf6a876a5a4bc4a48f2c9923fcbfc4",
      "max": 1017,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fad062f953014bac84e12360835164ac",
      "value": 1017
     }
    },
    "2452ea5f11a84f8199c23401fa7c055f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cc4eb243fff4e529d829bcbbfa19f63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3d5e426de3c44c8594e90c1a574dd916": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57fe87ff800d44e4936053202efb40b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd6a8412965a455a89b1146769f04c02",
       "IPY_MODEL_c383758e25c94d35912f136f36c087dd"
      ],
      "layout": "IPY_MODEL_5d68d7ce75164c7c99254c825e307a54"
     }
    },
    "5d68d7ce75164c7c99254c825e307a54": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7137008e45c84fb2a2ce8a0ece72cfdb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a4bbca4012e4ff28d2a1dd601aaf897": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2ac166380364eebb018c90da064f140",
      "placeholder": "​",
      "style": "IPY_MODEL_b64f31b66fef479bbb935bd978babf5d",
      "value": " 44.7M/44.7M [00:02&lt;00:00, 17.4MB/s]"
     }
    },
    "7ba7b1c4fbc64003ad9a82d3504afb4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c01d10eb78c44dad9eded65a3fa4d0f1",
       "IPY_MODEL_7a4bbca4012e4ff28d2a1dd601aaf897"
      ],
      "layout": "IPY_MODEL_05bb550af7304811a65569f01d32b18e"
     }
    },
    "82e56581231e445d9a4475f0ff496d01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9432f3eb3cee4a58bbe3da8530e81736": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a63e2652392749a8a3cc1784cccd0f0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2452ea5f11a84f8199c23401fa7c055f",
      "placeholder": "​",
      "style": "IPY_MODEL_9432f3eb3cee4a58bbe3da8530e81736",
      "value": " 1017/1017 [1:32:27&lt;00:00,  5.45s/it]"
     }
    },
    "b64f31b66fef479bbb935bd978babf5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c01d10eb78c44dad9eded65a3fa4d0f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d5e426de3c44c8594e90c1a574dd916",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0736205c4ab429cb6e08211323199cb",
      "value": 46827520
     }
    },
    "c383758e25c94d35912f136f36c087dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7137008e45c84fb2a2ce8a0ece72cfdb",
      "placeholder": "​",
      "style": "IPY_MODEL_08bda76b602f4c56a08fca6217e5d1f3",
      "value": " 4709/4709 [2:02:17&lt;00:00,  1.56s/it]"
     }
    },
    "dd6a8412965a455a89b1146769f04c02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82e56581231e445d9a4475f0ff496d01",
      "max": 4709,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2cc4eb243fff4e529d829bcbbfa19f63",
      "value": 4709
     }
    },
    "e0736205c4ab429cb6e08211323199cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e2ac166380364eebb018c90da064f140": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7519b62f0574195b9f85f1b77342ef7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2015a3677fdb456a81ef9d0dd4c3aa03",
       "IPY_MODEL_a63e2652392749a8a3cc1784cccd0f0a"
      ],
      "layout": "IPY_MODEL_17a0a356cd1c4c52b8d2ac36a5f0cd92"
     }
    },
    "fad062f953014bac84e12360835164ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fdcf6a876a5a4bc4a48f2c9923fcbfc4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
